{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load my models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init from pretrained model\n"
     ]
    }
   ],
   "source": [
    "from torchcore.util import Config\n",
    "from torchcore.dnn.networks.detectors.build import build_detector\n",
    "cfg = 'configs/retinanet/retinanet_resnet50_fpn_coco.py'\n",
    "config = Config.fromfile(cfg)\n",
    "model=build_detector(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcore.data.datasets.build import build_dataloader\n",
    "from torchcore.util.config import Config\n",
    "config.dataloader_val.batch_size=1\n",
    "val_dataset_loader = build_dataloader(config.dataloader_val,distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.14s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.expanduser('~/data/datasets/Fashionpedia/annotations/instances_attributes_val2020.json')\n",
    "\n",
    "names = val_dataset_loader.dataset.get_coco_style_names(json_path, with_cat_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torchcore.tools.visulize_tools import draw_plain_boxes, draw_single_image\n",
    "from torchcore.tools.visulize_tools import visulize_heatmaps_with_image, visulize_colored_heatmaps_with_image\n",
    "from torchcore.tools.color_gen import random_colors\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from torchcore.data.util import set_device\n",
    "\n",
    "dataset= val_dataset_loader.dataset\n",
    "score_thresh = 0.6\n",
    "colors = random_colors(46)\n",
    "#model.eval()\n",
    "\n",
    "for i, (inputs, targets) in enumerate(val_dataset_loader):\n",
    "    #results = model(inputs, targets)\n",
    "    inputs1 = set_device(inputs, 1)\n",
    "    targets1 = set_device(targets,1)\n",
    "    break\n",
    "\n",
    "for i, (inputs, targets) in enumerate(val_dataset_loader):\n",
    "    inputs2 = set_device(inputs, 2)\n",
    "    targets2 = set_device(targets,2)\n",
    "    break\n",
    "    input, target = dataset[i]\n",
    "    im = input['data']\n",
    "    boxes = results['boxes'][0].detach().numpy()\n",
    "    labels= results['labels'][0].detach().numpy()-1\n",
    "    scores = results['scores'][0].detach().numpy()\n",
    "\n",
    "    keep = scores > score_thresh\n",
    "    boxes = boxes[keep]\n",
    "    labels = labels[keep]\n",
    "    scores = scores[keep]\n",
    "    draw_plain_boxes(im, boxes)\n",
    "    draw_single_image(im, boxes, scores, labels, colors, class_names=names)\n",
    "    #heatmap = class_hp[fpn_layer][0].detach().numpy()\n",
    "    #heatmap = centerness_hp[fpn_layer][0].detach().numpy()\n",
    "    #mixim=visulize_colored_heatmaps_with_image(heatmap, im)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(im)\n",
    "\n",
    "    if i>-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_mem():\n",
    "    print('1:',torch.cuda.memory_allocated(1))\n",
    "    print('max 1:',torch.cuda.max_memory_allocated(1))\n",
    "    print('2:',torch.cuda.memory_allocated(2))\n",
    "    print('max 2:',torch.cuda.max_memory_allocated(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 10:14:17,515 - mmcv - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}\n",
      "2022-02-09 10:14:17,517 - mmcv - INFO - load model from: torchvision://resnet50\n",
      "2022-02-09 10:14:17,517 - mmcv - INFO - Use load_from_torchvision loader\n",
      "2022-02-09 10:14:17,693 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n",
      "2022-02-09 10:14:17,715 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}\n",
      "2022-02-09 10:14:17,762 - mmcv - INFO - initialize RetinaHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'retina_cls', 'std': 0.01, 'bias_prob': 0.01}}\n",
      "2022-02-09 10:14:17,865 - mmcv - INFO - \n",
      "backbone.conv1.weight - torch.Size([64, 3, 7, 7]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,866 - mmcv - INFO - \n",
      "backbone.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,866 - mmcv - INFO - \n",
      "backbone.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,867 - mmcv - INFO - \n",
      "backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,869 - mmcv - INFO - \n",
      "backbone.layer1.0.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,869 - mmcv - INFO - \n",
      "backbone.layer1.0.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,870 - mmcv - INFO - \n",
      "backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,871 - mmcv - INFO - \n",
      "backbone.layer1.0.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,872 - mmcv - INFO - \n",
      "backbone.layer1.0.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,872 - mmcv - INFO - \n",
      "backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,873 - mmcv - INFO - \n",
      "backbone.layer1.0.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,873 - mmcv - INFO - \n",
      "backbone.layer1.0.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,874 - mmcv - INFO - \n",
      "backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,875 - mmcv - INFO - \n",
      "backbone.layer1.0.downsample.1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,876 - mmcv - INFO - \n",
      "backbone.layer1.0.downsample.1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,876 - mmcv - INFO - \n",
      "backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,877 - mmcv - INFO - \n",
      "backbone.layer1.1.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,877 - mmcv - INFO - \n",
      "backbone.layer1.1.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,878 - mmcv - INFO - \n",
      "backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,878 - mmcv - INFO - \n",
      "backbone.layer1.1.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,880 - mmcv - INFO - \n",
      "backbone.layer1.1.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,880 - mmcv - INFO - \n",
      "backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,881 - mmcv - INFO - \n",
      "backbone.layer1.1.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,881 - mmcv - INFO - \n",
      "backbone.layer1.1.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,882 - mmcv - INFO - \n",
      "backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,882 - mmcv - INFO - \n",
      "backbone.layer1.2.bn1.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,882 - mmcv - INFO - \n",
      "backbone.layer1.2.bn1.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,884 - mmcv - INFO - \n",
      "backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,884 - mmcv - INFO - \n",
      "backbone.layer1.2.bn2.weight - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,885 - mmcv - INFO - \n",
      "backbone.layer1.2.bn2.bias - torch.Size([64]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,885 - mmcv - INFO - \n",
      "backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,886 - mmcv - INFO - \n",
      "backbone.layer1.2.bn3.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,886 - mmcv - INFO - \n",
      "backbone.layer1.2.bn3.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,887 - mmcv - INFO - \n",
      "backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,887 - mmcv - INFO - \n",
      "backbone.layer2.0.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,888 - mmcv - INFO - \n",
      "backbone.layer2.0.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,888 - mmcv - INFO - \n",
      "backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,889 - mmcv - INFO - \n",
      "backbone.layer2.0.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,889 - mmcv - INFO - \n",
      "backbone.layer2.0.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,890 - mmcv - INFO - \n",
      "backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,890 - mmcv - INFO - \n",
      "backbone.layer2.0.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,891 - mmcv - INFO - \n",
      "backbone.layer2.0.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,892 - mmcv - INFO - \n",
      "backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,892 - mmcv - INFO - \n",
      "backbone.layer2.0.downsample.1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,893 - mmcv - INFO - \n",
      "backbone.layer2.0.downsample.1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,893 - mmcv - INFO - \n",
      "backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,894 - mmcv - INFO - \n",
      "backbone.layer2.1.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,894 - mmcv - INFO - \n",
      "backbone.layer2.1.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,895 - mmcv - INFO - \n",
      "backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,895 - mmcv - INFO - \n",
      "backbone.layer2.1.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,897 - mmcv - INFO - \n",
      "backbone.layer2.1.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,898 - mmcv - INFO - \n",
      "backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,898 - mmcv - INFO - \n",
      "backbone.layer2.1.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,899 - mmcv - INFO - \n",
      "backbone.layer2.1.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,899 - mmcv - INFO - \n",
      "backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,900 - mmcv - INFO - \n",
      "backbone.layer2.2.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,900 - mmcv - INFO - \n",
      "backbone.layer2.2.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,901 - mmcv - INFO - \n",
      "backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,901 - mmcv - INFO - \n",
      "backbone.layer2.2.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,902 - mmcv - INFO - \n",
      "backbone.layer2.2.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,902 - mmcv - INFO - \n",
      "backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,903 - mmcv - INFO - \n",
      "backbone.layer2.2.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,903 - mmcv - INFO - \n",
      "backbone.layer2.2.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,904 - mmcv - INFO - \n",
      "backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,904 - mmcv - INFO - \n",
      "backbone.layer2.3.bn1.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,905 - mmcv - INFO - \n",
      "backbone.layer2.3.bn1.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,906 - mmcv - INFO - \n",
      "backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,906 - mmcv - INFO - \n",
      "backbone.layer2.3.bn2.weight - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,907 - mmcv - INFO - \n",
      "backbone.layer2.3.bn2.bias - torch.Size([128]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,907 - mmcv - INFO - \n",
      "backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,908 - mmcv - INFO - \n",
      "backbone.layer2.3.bn3.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,908 - mmcv - INFO - \n",
      "backbone.layer2.3.bn3.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,909 - mmcv - INFO - \n",
      "backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,909 - mmcv - INFO - \n",
      "backbone.layer3.0.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,910 - mmcv - INFO - \n",
      "backbone.layer3.0.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,910 - mmcv - INFO - \n",
      "backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,911 - mmcv - INFO - \n",
      "backbone.layer3.0.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,912 - mmcv - INFO - \n",
      "backbone.layer3.0.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,912 - mmcv - INFO - \n",
      "backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,913 - mmcv - INFO - \n",
      "backbone.layer3.0.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,913 - mmcv - INFO - \n",
      "backbone.layer3.0.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,914 - mmcv - INFO - \n",
      "backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,914 - mmcv - INFO - \n",
      "backbone.layer3.0.downsample.1.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,915 - mmcv - INFO - \n",
      "backbone.layer3.0.downsample.1.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,915 - mmcv - INFO - \n",
      "backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,916 - mmcv - INFO - \n",
      "backbone.layer3.1.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,916 - mmcv - INFO - \n",
      "backbone.layer3.1.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,917 - mmcv - INFO - \n",
      "backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,918 - mmcv - INFO - \n",
      "backbone.layer3.1.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,918 - mmcv - INFO - \n",
      "backbone.layer3.1.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,919 - mmcv - INFO - \n",
      "backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,919 - mmcv - INFO - \n",
      "backbone.layer3.1.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,920 - mmcv - INFO - \n",
      "backbone.layer3.1.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,920 - mmcv - INFO - \n",
      "backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,921 - mmcv - INFO - \n",
      "backbone.layer3.2.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,921 - mmcv - INFO - \n",
      "backbone.layer3.2.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,922 - mmcv - INFO - \n",
      "backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,922 - mmcv - INFO - \n",
      "backbone.layer3.2.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,926 - mmcv - INFO - \n",
      "backbone.layer3.2.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,927 - mmcv - INFO - \n",
      "backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,927 - mmcv - INFO - \n",
      "backbone.layer3.2.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,928 - mmcv - INFO - \n",
      "backbone.layer3.2.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,928 - mmcv - INFO - \n",
      "backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,929 - mmcv - INFO - \n",
      "backbone.layer3.3.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,929 - mmcv - INFO - \n",
      "backbone.layer3.3.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,930 - mmcv - INFO - \n",
      "backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,930 - mmcv - INFO - \n",
      "backbone.layer3.3.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,931 - mmcv - INFO - \n",
      "backbone.layer3.3.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,931 - mmcv - INFO - \n",
      "backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,931 - mmcv - INFO - \n",
      "backbone.layer3.3.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,932 - mmcv - INFO - \n",
      "backbone.layer3.3.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,933 - mmcv - INFO - \n",
      "backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,934 - mmcv - INFO - \n",
      "backbone.layer3.4.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,934 - mmcv - INFO - \n",
      "backbone.layer3.4.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,935 - mmcv - INFO - \n",
      "backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,935 - mmcv - INFO - \n",
      "backbone.layer3.4.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,936 - mmcv - INFO - \n",
      "backbone.layer3.4.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,936 - mmcv - INFO - \n",
      "backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,937 - mmcv - INFO - \n",
      "backbone.layer3.4.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,937 - mmcv - INFO - \n",
      "backbone.layer3.4.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,938 - mmcv - INFO - \n",
      "backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,938 - mmcv - INFO - \n",
      "backbone.layer3.5.bn1.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,939 - mmcv - INFO - \n",
      "backbone.layer3.5.bn1.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,939 - mmcv - INFO - \n",
      "backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,940 - mmcv - INFO - \n",
      "backbone.layer3.5.bn2.weight - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,940 - mmcv - INFO - \n",
      "backbone.layer3.5.bn2.bias - torch.Size([256]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,941 - mmcv - INFO - \n",
      "backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,941 - mmcv - INFO - \n",
      "backbone.layer3.5.bn3.weight - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,942 - mmcv - INFO - \n",
      "backbone.layer3.5.bn3.bias - torch.Size([1024]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,942 - mmcv - INFO - \n",
      "backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,943 - mmcv - INFO - \n",
      "backbone.layer4.0.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,943 - mmcv - INFO - \n",
      "backbone.layer4.0.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,944 - mmcv - INFO - \n",
      "backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,945 - mmcv - INFO - \n",
      "backbone.layer4.0.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,945 - mmcv - INFO - \n",
      "backbone.layer4.0.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,945 - mmcv - INFO - \n",
      "backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,946 - mmcv - INFO - \n",
      "backbone.layer4.0.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,946 - mmcv - INFO - \n",
      "backbone.layer4.0.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,947 - mmcv - INFO - \n",
      "backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,948 - mmcv - INFO - \n",
      "backbone.layer4.0.downsample.1.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,948 - mmcv - INFO - \n",
      "backbone.layer4.0.downsample.1.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,949 - mmcv - INFO - \n",
      "backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,949 - mmcv - INFO - \n",
      "backbone.layer4.1.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,949 - mmcv - INFO - \n",
      "backbone.layer4.1.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,950 - mmcv - INFO - \n",
      "backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,950 - mmcv - INFO - \n",
      "backbone.layer4.1.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,951 - mmcv - INFO - \n",
      "backbone.layer4.1.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,951 - mmcv - INFO - \n",
      "backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,951 - mmcv - INFO - \n",
      "backbone.layer4.1.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,952 - mmcv - INFO - \n",
      "backbone.layer4.1.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,952 - mmcv - INFO - \n",
      "backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,953 - mmcv - INFO - \n",
      "backbone.layer4.2.bn1.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,953 - mmcv - INFO - \n",
      "backbone.layer4.2.bn1.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,953 - mmcv - INFO - \n",
      "backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,954 - mmcv - INFO - \n",
      "backbone.layer4.2.bn2.weight - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,954 - mmcv - INFO - \n",
      "backbone.layer4.2.bn2.bias - torch.Size([512]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,955 - mmcv - INFO - \n",
      "backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,955 - mmcv - INFO - \n",
      "backbone.layer4.2.bn3.weight - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,955 - mmcv - INFO - \n",
      "backbone.layer4.2.bn3.bias - torch.Size([2048]): \n",
      "PretrainedInit: load from torchvision://resnet50 \n",
      " \n",
      "2022-02-09 10:14:17,958 - mmcv - INFO - \n",
      "neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,958 - mmcv - INFO - \n",
      "neck.lateral_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,958 - mmcv - INFO - \n",
      "neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,959 - mmcv - INFO - \n",
      "neck.lateral_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,959 - mmcv - INFO - \n",
      "neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,960 - mmcv - INFO - \n",
      "neck.lateral_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,960 - mmcv - INFO - \n",
      "neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,961 - mmcv - INFO - \n",
      "neck.fpn_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,962 - mmcv - INFO - \n",
      "neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,962 - mmcv - INFO - \n",
      "neck.fpn_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,962 - mmcv - INFO - \n",
      "neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,963 - mmcv - INFO - \n",
      "neck.fpn_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,963 - mmcv - INFO - \n",
      "neck.fpn_convs.3.conv.weight - torch.Size([256, 2048, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,964 - mmcv - INFO - \n",
      "neck.fpn_convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,964 - mmcv - INFO - \n",
      "neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,965 - mmcv - INFO - \n",
      "neck.fpn_convs.4.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,966 - mmcv - INFO - \n",
      "bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,966 - mmcv - INFO - \n",
      "bbox_head.cls_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,967 - mmcv - INFO - \n",
      "bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,967 - mmcv - INFO - \n",
      "bbox_head.cls_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,968 - mmcv - INFO - \n",
      "bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,968 - mmcv - INFO - \n",
      "bbox_head.cls_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,969 - mmcv - INFO - \n",
      "bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,969 - mmcv - INFO - \n",
      "bbox_head.cls_convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,970 - mmcv - INFO - \n",
      "bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,970 - mmcv - INFO - \n",
      "bbox_head.reg_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,971 - mmcv - INFO - \n",
      "bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,972 - mmcv - INFO - \n",
      "bbox_head.reg_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,972 - mmcv - INFO - \n",
      "bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,973 - mmcv - INFO - \n",
      "bbox_head.reg_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,973 - mmcv - INFO - \n",
      "bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,974 - mmcv - INFO - \n",
      "bbox_head.reg_convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of RetinaNet  \n",
      " \n",
      "2022-02-09 10:14:17,974 - mmcv - INFO - \n",
      "bbox_head.retina_cls.weight - torch.Size([720, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=-4.59511985013459 \n",
      " \n",
      "2022-02-09 10:14:17,975 - mmcv - INFO - \n",
      "bbox_head.retina_cls.bias - torch.Size([720]): \n",
      "NormalInit: mean=0, std=0.01, bias=-4.59511985013459 \n",
      " \n",
      "2022-02-09 10:14:17,975 - mmcv - INFO - \n",
      "bbox_head.retina_reg.weight - torch.Size([36, 256, 3, 3]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "2022-02-09 10:14:17,976 - mmcv - INFO - \n",
      "bbox_head.retina_reg.bias - torch.Size([36]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.runner import get_dist_info, init_dist\n",
    "from mmcv.utils import get_git_hash\n",
    "\n",
    "from mmdet import __version__\n",
    "from mmdet.apis import set_random_seed, train_detector\n",
    "from mmdet.datasets import build_dataset\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.utils import collect_env, get_root_logger\n",
    "\n",
    "mmconfig='mmconfigs/retinanet/retinanet_r50_fpn_1x_coco.py'\n",
    "mmcfg = Config.fromfile(mmconfig)\n",
    "mm_model = build_detector(\n",
    "        mmcfg.model,\n",
    "        train_cfg=mmcfg.get('train_cfg'),\n",
    "        test_cfg=mmcfg.get('test_cfg'))\n",
    "mm_model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(3):\n",
    "        model.neck.inner_blocks[i].weight.copy_(mm_model.neck.lateral_convs[i].conv.weight)\n",
    "        model.neck.layer_blocks[i].weight.copy_(mm_model.neck.fpn_convs[i].conv.weight)\n",
    "    model.neck.extra_blocks.p6.weight.copy_(mm_model.neck.fpn_convs[3].conv.weight)\n",
    "    model.neck.extra_blocks.p7.weight.copy_(mm_model.neck.fpn_convs[4].conv.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        model.det_head.head.cls_head[2*i].weight.copy_(mm_model.bbox_head.cls_convs[i].conv.weight)\n",
    "        model.det_head.head.bbox_head[2*i].weight.copy_(mm_model.bbox_head.reg_convs[i].conv.weight)\n",
    "    \n",
    "    model.det_head.head.cls_head[8].weight.copy_(mm_model.bbox_head.retina_cls.weight)\n",
    "    model.det_head.head.bbox_head[8].weight.copy_(mm_model.bbox_head.retina_reg.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 12583936\n",
      "max 1: 12583936\n",
      "2: 12583936\n",
      "max 2: 12583936\n",
      "1: 167852032\n",
      "max 1: 167852032\n",
      "2: 167852032\n",
      "max 2: 167852032\n"
     ]
    }
   ],
   "source": [
    "print_mem()\n",
    "model = model.to(1)\n",
    "mm_model = mm_model.to(2)\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 5079144448\n",
      "max 1: 5268480512\n",
      "2: 2568214016\n",
      "max 2: 2713717248\n",
      "tensor(80, device='cuda:1')\n",
      "1: 5078737408\n",
      "max 1: 6461556736\n",
      "2: 2569232896\n",
      "max 2: 3913680896\n"
     ]
    }
   ],
   "source": [
    "print_mem()\n",
    "model.train()\n",
    "loss = model(inputs1, targets1)\n",
    "mm_model.train()\n",
    "img_meta = [{'img_shape':imsize,'scale_factor':imscale,'flip':False} for imsize,imscale in zip(inputs2['image_sizes'],inputs2['image_sizes'])]\n",
    "for meta in img_meta:\n",
    "    meta['pad_shape'] = (inputs2['data'].shape[2],inputs2['data'].shape[3],inputs2['data'].shape[1])\n",
    "gt_boxes = [t['boxes'] for t in targets2]\n",
    "gt_labels = [t['labels']-1 for t in targets2]\n",
    "#the losses of five layers\n",
    "mm_loss=mm_model(inputs2['data'],img_meta,gt_bboxes=gt_boxes,gt_labels=gt_labels)\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_objectness': tensor(1.2170, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'loss_box_reg': tensor(0.7444, device='cuda:1', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_cls': tensor(1.2170, device='cuda:2', grad_fn=<AddBackward0>), 'loss_bbox': tensor(0.7444, device='cuda:2', grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "losses_sum={}\n",
    "for k,v in mm_loss.items():\n",
    "    losses_sum[k]=sum(v)\n",
    "print(losses_sum)\n",
    "mm_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 167852032\n",
      "2: 167852032\n",
      "1: 1094981632\n",
      "2: 1094981632\n"
     ]
    }
   ],
   "source": [
    "print_mem()\n",
    "model.train()\n",
    "out = model.backbone(inputs1['data'])\n",
    "mm_model.train()\n",
    "mm_out=mm_model.backbone(inputs2['data'])\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1094981632\n",
      "2: 1094981632\n",
      "1: 1136163840\n",
      "2: 1136188416\n"
     ]
    }
   ],
   "source": [
    "print_mem()\n",
    "out = model.neck(out)\n",
    "mm_out = mm_model.neck(mm_out)\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1136163840\n",
      "2: 1136188416\n",
      "1: 1364539392\n",
      "2: 1364366848\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mmdet.core import (anchor_inside_flags, build_assigner, build_bbox_coder,\n",
    "                        build_prior_generator, build_sampler, images_to_levels,\n",
    "                        multi_apply, unmap)\n",
    "print_mem()\n",
    "out_head = model.det_head.head(out)\n",
    "mm_head_out = multi_apply(mm_model.bbox_head.forward_single, mm_out)\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1364539392\n",
      "2: 1364366848\n",
      "1: 2004828672\n",
      "2: 2565340672\n"
     ]
    }
   ],
   "source": [
    "print_mem()\n",
    "\n",
    "img_meta = [{'img_shape':imsize,'scale_factor':imscale,'flip':False} for imsize,imscale in zip(inputs2['image_sizes'],inputs2['image_sizes'])]\n",
    "for meta in img_meta:\n",
    "    meta['pad_shape'] = (inputs2['data'].shape[2],inputs2['data'].shape[3],inputs2['data'].shape[1])\n",
    "gt_boxes = [t['boxes'] for t in targets2]\n",
    "gt_labels = [t['labels'] for t in targets2]\n",
    "#the losses of five layers\n",
    "losses=mm_model(inputs2['data'],img_meta,gt_bboxes=gt_boxes,gt_labels=gt_labels)\n",
    "loss = model.det_head(inputs1,out,targets1)\n",
    "print_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_cls': tensor(1.2190, device='cuda:2', grad_fn=<AddBackward0>), 'loss_bbox': tensor(0.7460, device='cuda:2', grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "losses_sum={}\n",
    "for k,v in losses.items():\n",
    "    losses_sum[k]=sum(v)\n",
    "print(losses_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_objectness': tensor(1.2186, device='cuda:1', grad_fn=<DivBackward0>),\n",
       " 'loss_box_reg': tensor(0.7505, device='cuda:1', grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_objectness': tensor(1.2173, grad_fn=<DivBackward0>), 'loss_box_reg': tensor(0.7434, grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "out = model.backbone(inputs['data'])\n",
    "out = model.neck(out)\n",
    "out = list(out.values())\n",
    "out_head = model.det_head.head(out)\n",
    "loss = model.det_head(inputs,out,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "my cls loss tensor(1.2173, grad_fn=<DivBackward0>)\n",
      "mm cls loss tensor(1.2173, grad_fn=<AddBackward0>)\n",
      "1\n",
      "my cls loss tensor(1.1516, grad_fn=<DivBackward0>)\n",
      "mm cls loss tensor(1.1514, grad_fn=<AddBackward0>)\n",
      "1\n",
      "my bbox loss tensor(0.6852, grad_fn=<DivBackward0>)\n",
      "mm bbox loss tensor(0.6852, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from torchcore.tools.visulize_tools import draw_plain_boxes, draw_single_image\n",
    "from torchcore.tools.visulize_tools import visulize_heatmaps_with_image, visulize_colored_heatmaps_with_image\n",
    "from torchcore.tools.color_gen import random_colors\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "dataset= val_dataset_loader.dataset\n",
    "score_thresh = 0.6\n",
    "colors = random_colors(46)\n",
    "model.train()\n",
    "mm_model.train()\n",
    "\n",
    "for i, (inputs, targets) in enumerate(val_dataset_loader):\n",
    "    loss = model(inputs, targets)\n",
    "    img_meta = [{'img_shape':imsize,'scale_factor':imscale,'flip':False} for imsize,imscale in zip(inputs['image_sizes'],inputs['image_sizes'])]\n",
    "    for meta in img_meta:\n",
    "        meta['pad_shape'] = (inputs['data'].shape[2],inputs['data'].shape[3],inputs['data'].shape[1])\n",
    "    gt_boxes = [t['boxes'] for t in targets]\n",
    "    gt_labels = [t['labels'] for t in targets]\n",
    "    #the losses of five layers\n",
    "    losses=mm_model(inputs['data'],img_meta,gt_bboxes=gt_boxes,gt_labels=gt_labels)\n",
    "    losses_sum={}\n",
    "    for k,v in losses.items():\n",
    "        losses_sum[k]=sum(v)\n",
    "    if loss['loss_objectness']-losses_sum['loss_cls']>1e-5:\n",
    "        print(i)\n",
    "        print('my cls loss',loss['loss_objectness'])\n",
    "        print('mm cls loss',losses_sum['loss_cls'])\n",
    "    if loss['loss_box_reg']!=losses_sum['loss_bbox']>1e-5:\n",
    "        print(i)\n",
    "        print('my bbox loss',loss['loss_box_reg'])\n",
    "        print('mm bbox loss',losses_sum['loss_bbox'])\n",
    "    if i>0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9605e-08, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss['loss_box_reg']-losses_sum['loss_bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss['loss_objectness']-losses_sum['loss_cls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_model.train()\n",
    "mm_out=mm_model.backbone(inputs['data'])\n",
    "mm_out=mm_model.neck(mm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mm_head_out=mm_model.bbox_head(mm_out)\n",
    "from mmdet.core import (anchor_inside_flags, build_assigner, build_bbox_coder,\n",
    "                        build_prior_generator, build_sampler, images_to_levels,\n",
    "                        multi_apply, unmap)\n",
    "mm_head_out = multi_apply(mm_model.bbox_head.forward_single, mm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_cls': tensor(1.2173, grad_fn=<AddBackward0>), 'loss_bbox': tensor(0.7434, grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "img_meta = [{'img_shape':imsize,'scale_factor':imscale,'flip':False} for imsize,imscale in zip(inputs['image_sizes'],inputs['image_sizes'])]\n",
    "for meta in img_meta:\n",
    "    meta['pad_shape'] = (inputs['data'].shape[2],inputs['data'].shape[3],inputs['data'].shape[1])\n",
    "gt_boxes = [t['boxes'] for t in targets]\n",
    "gt_labels = [t['labels'] for t in targets]\n",
    "#the losses of five layers\n",
    "losses=mm_model(inputs['data'],img_meta,gt_bboxes=gt_boxes,gt_labels=gt_labels)\n",
    "losses_sum={}\n",
    "for k,v in losses.items():\n",
    "    losses_sum[k]=sum(v)\n",
    "print(losses_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "featmap_sizes = [featmap.size()[-2:] for featmap in out]\n",
    "device = out[0].device\n",
    "\n",
    "mm_anchor,mm_valid = mm_model.bbox_head.get_anchors(featmap_sizes,img_meta,device=device)\n",
    "label_channels = mm_model.bbox_head.cls_out_channels if mm_model.bbox_head.use_sigmoid_cls else 1\n",
    "cls_reg_targets = mm_model.bbox_head.get_targets(\n",
    "    mm_anchor,\n",
    "    mm_valid,\n",
    "    gt_boxes,\n",
    "    img_meta,\n",
    "    gt_bboxes_ignore_list=None,\n",
    "    gt_labels_list=gt_labels,\n",
    "    label_channels=label_channels)\n",
    "(labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n",
    "         num_total_pos, num_total_neg) = cls_reg_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(136478)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(label_weights_list[0]==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'img_shape': torch.Size([800, 1201]),\n",
       "  'scale_factor': torch.Size([800, 1201]),\n",
       "  'flip': False,\n",
       "  'pad_shape': (800, 1216, 3),\n",
       "  'batch_input_shape': (800, 1216)}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([100, 152]),\n",
       " torch.Size([50, 76]),\n",
       " torch.Size([25, 38]),\n",
       " torch.Size([13, 19]),\n",
       " torch.Size([7, 10])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featmap_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_head_v=list(out_head.values())\n",
    "anchors = model.det_head.anchor_generater(inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-22.6274, -11.3137,  22.6274,  11.3137],\n",
       "        [-28.5088, -14.2544,  28.5088,  14.2544],\n",
       "        [-35.9188, -17.9594,  35.9188,  17.9594],\n",
       "        [-16.0000, -16.0000,  16.0000,  16.0000],\n",
       "        [-20.1587, -20.1587,  20.1587,  20.1587],\n",
       "        [-25.3984, -25.3984,  25.3984,  25.3984],\n",
       "        [-11.3137, -22.6274,  11.3137,  22.6274],\n",
       "        [-14.2544, -28.5088,  14.2544,  28.5088],\n",
       "        [-17.9594, -35.9188,  17.9594,  35.9188]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.det_head.anchor_generater.cell_anchors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-22.6274, -11.3137,  22.6274,  11.3137],\n",
       "        [-28.5088, -14.2544,  28.5088,  14.2544],\n",
       "        [-35.9188, -17.9594,  35.9188,  17.9594],\n",
       "        [-16.0000, -16.0000,  16.0000,  16.0000],\n",
       "        [-20.1587, -20.1587,  20.1587,  20.1587],\n",
       "        [-25.3984, -25.3984,  25.3984,  25.3984],\n",
       "        [-11.3137, -22.6274,  11.3137,  22.6274],\n",
       "        [-14.2544, -28.5088,  14.2544,  28.5088],\n",
       "        [-17.9594, -35.9188,  17.9594,  35.9188]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model.bbox_head.prior_generator.base_anchors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_anchor_all = [torch.cat(a) for a in mm_anchor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(anchors[0]-mm_anchor_all[0]==0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -22.6274,  -11.3137,   22.6274,   11.3137],\n",
       "        [ -28.5088,  -14.2544,   28.5088,   14.2544],\n",
       "        [ -35.9188,  -17.9594,   35.9188,   17.9594],\n",
       "        ...,\n",
       "        [ 970.9807,  405.9613, 1333.0193, 1130.0387],\n",
       "        [ 923.9299,  311.8599, 1380.0701, 1224.1401],\n",
       "        [ 864.6497,  193.2994, 1439.3503, 1342.7006]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_anchor_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([136800, 4])\n",
      "torch.Size([34200, 4])\n",
      "torch.Size([8550, 4])\n",
      "torch.Size([2223, 4])\n",
      "torch.Size([630, 4])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(mm_anchor[0][i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182403, 4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_anchor_all[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 800, 1216])\n",
      "2 torch.Size([1, 256, 100, 152])\n",
      "3 torch.Size([1, 256, 50, 76])\n",
      "p5 torch.Size([1, 256, 25, 38])\n",
      "p6 torch.Size([1, 256, 13, 19])\n",
      "p7 torch.Size([1, 256, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['data'].shape)\n",
    "for k,v in out.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 torch.Size([1, 720, 100, 152])\n",
      "3 torch.Size([1, 720, 50, 76])\n",
      "c5 torch.Size([1, 720, 25, 38])\n",
      "p6 torch.Size([1, 720, 13, 19])\n",
      "p7 torch.Size([1, 720, 7, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 720, 100, 152])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in out_head.items():\n",
    "    print(k,out_head[k][0].shape)\n",
    "out_head['2'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_objectness': tensor(1.2131, grad_fn=<DivBackward0>),\n",
       " 'loss_box_reg': tensor(0.7660, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mm_head_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,k in enumerate(['2','3','c5','p6','p7']):\n",
    "    for j in range(2):\n",
    "        if not (out_head[i][j]==mm_head_out[j][i]).all():\n",
    "            print(k,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36, 50, 76])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mm_head_out)\n",
    "for o in mm_head_out:\n",
    "#    print(o[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out['p7']-mm_out[4]==0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPN(\n",
       "  (lateral_convs): ModuleList(\n",
       "    (0): ConvModule(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): ConvModule(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ConvModule(\n",
       "      (conv): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (fpn_convs): ModuleList(\n",
       "    (0): ConvModule(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): ConvModule(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (2): ConvModule(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (3): ConvModule(\n",
       "      (conv): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (4): ConvModule(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")\n",
       "init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model.neck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0089]],\n",
       "\n",
       "         [[-0.0146]],\n",
       "\n",
       "         [[ 0.0061]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0321]],\n",
       "\n",
       "         [[ 0.0815]],\n",
       "\n",
       "         [[ 0.0473]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0414]],\n",
       "\n",
       "         [[ 0.0649]],\n",
       "\n",
       "         [[ 0.0858]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0172]],\n",
       "\n",
       "         [[-0.0727]],\n",
       "\n",
       "         [[ 0.0785]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0290]],\n",
       "\n",
       "         [[ 0.0455]],\n",
       "\n",
       "         [[-0.0305]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0734]],\n",
       "\n",
       "         [[ 0.0244]],\n",
       "\n",
       "         [[-0.0645]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.0556]],\n",
       "\n",
       "         [[ 0.0828]],\n",
       "\n",
       "         [[ 0.0670]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0353]],\n",
       "\n",
       "         [[ 0.0197]],\n",
       "\n",
       "         [[ 0.0801]]],\n",
       "\n",
       "\n",
       "        [[[-0.0222]],\n",
       "\n",
       "         [[-0.0245]],\n",
       "\n",
       "         [[ 0.0428]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0134]],\n",
       "\n",
       "         [[-0.0622]],\n",
       "\n",
       "         [[-0.0555]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0759]],\n",
       "\n",
       "         [[-0.0230]],\n",
       "\n",
       "         [[-0.0825]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0664]],\n",
       "\n",
       "         [[ 0.0710]],\n",
       "\n",
       "         [[-0.0366]]]], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model.neck.lateral_convs[0].conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 100, 152])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['2']-mm_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=model.backbone.conv1(inputs['data'])\n",
    "x = model.backbone.bn1(x)\n",
    "x = model.backbone.relu(x)\n",
    "x = model.backbone.maxpool(x)\n",
    "x = model.backbone.layer1(x)\n",
    "x = model.backbone.layer2[0].conv1(x)\n",
    "x = model.backbone.layer2[0].bn1(x)\n",
    "\n",
    "mm_model.train()\n",
    "mx=mm_model.backbone.conv1(inputs['data'])\n",
    "mx = mm_model.backbone.norm1(mx)\n",
    "mx = mm_model.backbone.relu(mx)\n",
    "mx = mm_model.backbone.maxpool(mx)\n",
    "mx = mm_model.backbone.layer1(mx)\n",
    "mx = mm_model.backbone.layer2[0].conv1(mx)\n",
    "mx = mm_model.backbone.layer2[0].norm1(mx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mx==x).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone.layer2[0].bn1.training\n",
    "#model.backbone.bn1.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model.backbone.layer2[0].norm1.training\n",
    "#mm_model.backbone.norm1.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_model.backbone.norm_eval"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0aa8413314c4b22e0108e3e0a06a48ff88ea70e28035422d69764e57470655e9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
